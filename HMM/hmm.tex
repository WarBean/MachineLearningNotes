\documentclass[11pt,a4paper]{article}
\usepackage{fontspec}
\usepackage[BoldFont, SlantFont, CJKnumber]{xeCJK}
\setCJKmainfont[BoldFont=Adobe Heiti Std R]{Adobe Song Std L}
\setCJKsansfont[BoldFont=Adobe Heiti Std R]{Adobe Kaiti Std R}
\setCJKmonofont{Adobe Fangsong Std R}
\XeTeXlinebreaklocale "zh" 
\XeTeXlinebreakskip = 0pt plus 1pt minus 0.1pt

% 数学公式相关
\usepackage{amsmath, bm}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\boldvec}[1]{\bm{#1}}
\newcommand*{\scale}[2][4]{\scalebox{#1}{$#2$}}
\newcommand*{\resize}[2]{\resizebox{#1}{!}{$#2$}}
\numberwithin{equation}{section}
\usepackage{relsize} % 为了在cases环境中显示大的求和号、求积号

% 首行缩进
\usepackage{indentfirst}
\setlength\parindent{2em}

\begin{document} 

% 标题、目录
\title{HMM笔记} 
\author{郑华滨} 
\date{2015.5.28}\maketitle
\renewcommand\contentsname{目\quad 录}
\tableofcontents
\newpage

\section{引言}

本文介绍了最基本的隐马尔可夫模型(Hidden Markov Model, HMM)的模型定义、模型使用算法、模型训练算法。其中，模型使用所用到的前向－后向算法(forward-backward algorithm)、维特比算法(Viterbi algorithm)、模型训练所用到的Expectation Maximization算法(EM algorithm)都会有详细的数学推导。

本文的第二节对HMM模型进行了形象化的、形式化的描述，第三节先界定了关于HMM模型使用、模型训练的三个问题，第四、五、六节分别解决第一、第三、第二个问题，其中第四节涉及前向算法和后向算法，第五节涉及Expectation Maximization算法，第六节涉及前向－后向算法和维特比算法。第七节是全篇总结。

\section{模型描述}

HMM模型是用来处理序列化数据(sequential data)的一种模型，对于一个观察到的长度为$ T $序列：$ o = o_1 o_2 o_3 \ldots o_T $，HMM假设有一个与之对应的内在的、本质的隐藏序列$ s = s_1 s_2 s_3 \ldots s_T $，两者之间一一对应，我们把前者称为观察变量构成的观察序列(observation)，后者称为状态变量构成的状态序列(state)。状态序列所代表的是我们所没有观察到的本质变化，而观察序列是作为状态序列的结果“发射”(emit)出来的，当然这只是直观的解释，在实际的数学推导上观察序列只是作为一组隐含变量(latent variable)引入原来的图模型而已。

一个经常举的例子是，我们可以把天气变化看作一个观察序列，例如“晴阴晴雨雨阴雷……”，同时假设有另外一个隐藏序列“abbedf……”，与之一一对应。不过，我们只关注这些“abcd”的数学意义，并不一定要有实际的物理意义，当然你高兴的话可以为它们引入宗教意义，例如a表示这天是北欧雷神托尔值班，b表示这天是中国雷神雷震子值班，c表示今天是中国雨神萧敬腾值班，然后从abc到阴晴雨的过程，则是一个根据值班人员心情概率随机决定今天天气的过程……

除了状态序列这个最基本的假设之外，HMM模型还假设了以下三点：
\begin{itemize}
\item 每一个观察变量仅仅依赖于与之对应的那个状态变量，这叫“一一对应性”，用一组条件概率$ P(o_t | s_t) $来表示这种依赖关系；
\item 每一个状态变量仅仅依赖于它之前的状态变量（一般只依赖于前一个），这叫“马尔可夫性”，其中只依赖于前一个则叫“一阶马尔可夫性”，具体到一阶(first order)的情况，用一组条件概率$ P(s_{t + 1} | s_t) $来表示这种依赖关系；
\item 上述的条件概率分布中含有变量$ t $，然而它们并不随时间变化，例如不管哪一天轮到托尔值班，打雷的概率都是99\%，而不会今天是99\%明天是88\%，这叫“时序平稳性”。
\end{itemize}

下面给出具体的数学描述。

\subsection{符号}

序列长度：$ T $

一个状态序列：$ s = s_1 s_2 \ldots s_T $

一个观察序列：$ o = o_1 o_2 \ldots o_T $

状态符号数：$ N $

观察符号数：$ M $

状态符号集：$ S = \{S_1, S_2, \ldots S_N\}, \quad s_t \in S, \quad \text{当} 1 \le t \le T $

观察符号集：$ O = \{O_1, O_2, \ldots O_M\}, \quad o_t \in O, \quad 1 \text{当} \le t \le T $

转移概率(transition probability)：$ P(s_{t + 1} | s_t), \quad \text{当} 1 \le t \le T - 1 $

发射概率(emission probability)：$ P(o_t | s_t), \quad \text{当} 1 \le t \le T $

初始概率(initial probability)：$ P(s_1) $

\subsection{模型假设}

观察序列和状态序列之间的一一对应性，即每个观察变量仅仅依赖于与之对应的状态变量：
\begin{equation}
P(o_t | o_1, o_2, \ldots o_{t - 1}, o_{t + 1}, \ldots o_T, s_1, s_2, \ldots s_T) = P(o_t | s_t)
\end{equation}

状态序列的一阶马尔可夫性，即每个状态变量仅仅依赖于上一个状态变量：
\begin{equation}
P(s_t | s_1, s_2, \ldots s_{t - 1}) = P(s_t | s_{t - 1}), \quad \text{当} 2 \le t \le T 
\end{equation}

时序平稳性，即转移概率分布和发射概率分布不随时间变化：
\begin{equation}
P(s_{t_1} | s_{t_1 - 1}) = P(s_{t_2} | s_{t_2 - 1}), \quad \text{当} s_{t_1} = s_{t_2}, s_{t_1 - 1} = s_{t_2 - 1}, 2 \le t_1, t_2 \le T 
\end{equation}

\subsection{模型参数}

一般来说，状态符号和观察符号的个数$N$和$M$是由具体的问题场景预先确定下来了的，所以我们感兴趣的HMM模型参数主要是转移概率、发射概率以及初始概率：
\begin{subequations}
\begin{align}
a_{ij} & = P(s_t = S_j | s_{t - 1} = S_i)\\
b_{jk} & = P(o_t = O_k | s_t = S_j)\\
\pi_i & = P(s_1 = S_i)
\end{align}
\end{subequations}
其中$ 1 \le i, j \le N $，$ 1 \le k \le M $，$ 2 \le t \le T $，故所有的$ a_{ij} $构成矩阵$ A_{N \times N} $，所有的$ b_{jk} $构成矩阵$ B_{N \times M} $，所有的$ \pi_i $构成向量$ \pi_{N \times 1} $，记三元组：
\begin{equation}
\lambda = (A, B, \pi)
\end{equation}
这就表示HMM模型的所有参数。

\section{三个基本问题}

当我们定义了如上所述的HMM模型之后，要用它来干一些有意义的事情之前，要先解决三个基本问题：
\begin{itemize}
\item 已知一组模型参数$ \lambda = (A, B, \pi) $，给定一个观察序列一个观察序列$ o = o_1 o_2 ... o_T $，如何\textbf{高效地}计算出现的概率$ P(o | \lambda) $？
\item 已知一组模型参数$ \lambda = (A, B, \pi) $，给定一个观察序列一个观察序列$ o = o_1 o_2 ... o_T $，如何按照某种有意义的准则来找出一个状态序列$ s = s_1 s_2 ... s_T $，使之能够最好地“解释该观察序列的出现”？
\item 给定一个观察序列一个观察序列$ o = o_1 o_2 ... o_T $，如何求使这个观察序列出现概率最大的一组模型参数$ \lambda_0 = \argmax_{\lambda} P(o | \lambda) $？
\end{itemize}

第二个问题属于模型使用问题，例如在词性标注(part-of-speech tagging)问题中，给定一个句子，也就是一个词序列，我们关心应该给每个词标上什么样的词性，如动词、名词、形容词，如果我们把词序列作为观察序列，词性序列作为状态序列，那就刚好对应到第二个问题。第三个问题属于模型训练问题，或者说参数优化问题(parameter optimization)。第一个问题在我所知的范围内，并没有作为一个模型使用问题而出现，而是作为在解决第三个问题的过程中需要解决的子问题，可能在其他一些任务中（例如语音识别、笔迹识别就用到了HMM）就是作为模型使用问题存在的。

参考文献？？？是按照上面列出的顺序提出和解决这三个问题的，但是由于第一和第三个问题联系紧密，而第二个问题和它们关系不大，所以本文将适当调整讲述顺序。

\section{第一个问题}

\subsection{暴力算法}

现在，有了模型参数$ \lambda $，有了观察序列$ o = o_1 o_2 ... o_T $，我们可以先假设知道状态序列$ s = s_1 s_2 ... s_T $，可以容易写出条件概率$ P(o | s, \lambda) $和$ P(s | \lambda) $，由这两个条件概率可以写出$ P(o, s | \lambda) $：
\begin{align}
\begin{split}
P(o | s, \lambda) 
& = \prod_{t = 1}^T P(o_t | s, \lambda) = \prod_{t = 1}^T P(o_t | s_t, \lambda) = \prod_{t = 1}^T B(s_t, o_t)
\end{split}
\\
\begin{split}
P(s | \lambda) \quad
& = P(s_1 | \lambda) \prod_{t = 2}^T P(s_t | s_{t - 1}, \lambda) = \pi(s_1) \prod_{t = 2}^T A(s_{t - 1}, s_t)
\end{split}
\\
\begin{split}
P(o, s | \lambda)
& = P(o | s, \lambda) P(s | \lambda) = \pi(s_1) B(s_1, o_1) \prod_{t = 2}^T A(s_{t - 1}, s_t) B(s_t, o_t)
\end{split}
\end{align}
其中对于函数$ A(\cdot, \cdot) $、$ B(\cdot, \cdot) $、$ \pi(\cdot) $，有：
\begin{subequations}
\begin{align}
A(S_i, S_j) & = a_{ij}\\
B(S_j, O_k) & = b_{jk}\\
\pi(S_i) & = a_{i}
\end{align}
\end{subequations}

事实上我们并不知道状态序列是什么，而每一种状态序列都有可能，因此需要对整个状态序列空间求和，把$ P(o, s | \lambda) $中的$ s $"margin out"掉，即$ P(o | \lambda) = \sum_s P(o, s| \lambda) $，展开后有：
\begin{equation}\label{P(o|lambda)}
P(o | \lambda) = \sum_{s_1, s_2, ... s_T} \pi(s_1) B(s_1, o_1) \prod_{t = 2}^T A(s_{t - 1}, s_t) B(s_t, o_t)
\end{equation}
分析这个运算的复杂度：总共需要进行$O(N^T)$规模的求和，每个求和需要做$ O(T) $规模的乘积，总的复杂度是$ O(TN^T) $，这显然是不行的。

\subsection{前向算法}

首先注意到，按照乘法分配律有：
\begin{equation}
\sum_{x = 1}^N f(x, y)g(y) = g(y) \sum_{x = 1}^N f(x, y)
\end{equation}

观察式子\eqref{P(o|lambda)}发现有类似的结构，同时注意到它做了很多重复计算，例如$\pi(s_1)$跟$s_2$无关，却要在$s_2$上求和时重复地计算，更不要说后面的$s_3,s_4...$了，所以先想办法用乘法分配律先提出共同的因子。先尝试从从$s_1$开始，从前到后：
\begin{equation}\label{forward_reduce}
P(o | \lambda) = \sum_{s_T} B(s_T, o_T) \prod_{t = T - 1}^1 \sum_{s_t} A(s_t, s_{t + 1}) B(s_t, o_t) \pi(s_1)
\end{equation}

似乎复杂度降低了不少，但是这个式子在算法实现的角度上不好算，进一步观察发现它具有一定的周期递推特性，所以接下来通过引入一组新的函数$\alpha_t$来写出一个递归形式的表示：
\begin{equation}\label{alpha_t_definition}
\alpha_t(s_t) = 
	\begin{cases}
		B(s_1, o_1) \pi(s_1) & \text{当} t = 1\\
		B(s_t, o_t) \mathlarger{\prod}_{u = t - 1}^1 \mathlarger{\sum}_{s_u} A(s_u, s_{u + 1}) B(s_u, o_u) \pi(s_1) & \text{当} 2 \le t \le T
	\end{cases}
\end{equation}
其实就是把式子\eqref{forward_reduce}从后往前“截断”到$B(s_t, o_t)$左边的部分。按照定义，$\alpha_t(s_t)$具有如下递推性质：
\begin{equation}\label{alpha_t_recursive}
\alpha_t(s_t) = B(s_t, o_t) \sum_{s_{t - 1}} A(s_{t - 1}, s_t) \alpha_{t - 1}(s_{t - 1}) \quad \text{当} 2 \le t \le T
\end{equation}
这样从算法实现的角度就可以不断迭代从$\alpha_1(s_1)$一直迭代算到$\alpha_T(s_T)$，然后再最后跨一步算$p(o | \lambda)$：
\begin{equation}\label{alpha_t_final_shot}
p(o | \lambda) = \sum_{s_T} \alpha_T(s_T)
\end{equation}

考察上述计算的复杂度，$t = 1$时不用计算，从$t = 2 \rightarrow T$共$T - 1$次迭代，每次迭代按照递归公式\eqref{alpha_t_recursive}其实是这样的一个矩阵运算：
\begin{equation}
\boldvec{\alpha_t} = A^T \cdot \boldvec{\alpha_{t - 1}} \circ \boldvec{b(o_t)}
\end{equation}
其中，$\circ$表示阿玛达乘积(Hadamard product)，即逐位相乘,列向量$\boldvec{\alpha_t}$、$\boldvec{b(o_t)}$分别为：
\begin{subequations}
\begin{align}
\boldvec{\alpha_t} & = [\alpha_t(S_1), \alpha_t(S_2), ... \alpha_t(S_N)]^T\\
\boldvec{b(o_t)} & = [B(S_1, o_t), B(S_2, o_t), ... B(S_N, o_t)]^T
\end{align}
\end{subequations}
故而每次迭代的复杂度是$ O(N^2) $，迭代$ T - 1 $次，最后一次算$ p(o | \lambda) $复杂度是$ O(N) $，总的复杂度是$ O(TN^2) $，比暴力算法的$O(TN^T)$高不知道哪去了

刚刚是把式\eqref{forward_reduce}从后往前“截断”到$B(s_t, o_t)$左边，但其实也可以“截断”到$B(s_t, o_t)$右边。定义一个新的函数$\alpha'_t$：
\begin{equation}\label{alpha'_t_definition}
\alpha'_t(s_t) = 
	\begin{cases}
		\pi(s_1) & \text{当} t = 1\\
		\mathlarger{\prod}_{u = t - 1}^1 \mathlarger{\sum}_{s_u} A(s_u, s_{u + 1}) B(s_u, o_u) \pi(s_1) & \text{当} 2 \le t \le T
	\end{cases}
\end{equation}
其递推性质为：
\begin{equation}\label{alpha'_t_recursive}
\alpha'_t(s_t) = \sum_{s_{t - 1}} A(s_{t - 1}, s_t) B(s_{t - 1}, o_{t - 1}) \alpha_{t - 1}(s_{t - 1}) \quad \text{当} 2 \le t \le T
\end{equation}
从$ \alpha'_1(s_1) $一直迭代算到$ \alpha'_T(s_T) $，然后再最后跨一步算$ p(o | \lambda) $：
\begin{equation}\label{alpha'_t_final_shot}
p(o | \lambda) = \sum_{s_T} B(s_T, o_t) \alpha'_T(s_T)
\end{equation}
式\eqref{alpha'_t_definition}、\eqref{alpha'_t_recursive}、\eqref{alpha'_t_final_shot}与式\eqref{alpha_t_definition}、\eqref{alpha_t_recursive}、\eqref{alpha_t_final_shot}表示的计算过程是等价的，复杂度同样也是$ O(TN^2) $。

推导到这里，可以问一个问题：$ \alpha_t $和$ \alpha'_t $是否具有某种概率意义？后面我们会发现，这个问题非常重要。

先从$\alpha_t$开始，观察其定义\eqref{alpha_t_definition}，发现：
\begin{align}
\begin{split}\label{induction_init}
\alpha_1(s_1) & = B(s_1, o_1) \pi(s_1) \\
& = P(o_1 | s_1, \lambda) P(s_1 | \lambda) \\ 
& = P(o_1, s_1 | \lambda)
\end{split}
\\
\begin{split}
\alpha_2(s_2) & = B(s_2, o_2) \sum_{s_1} A(s_1, s_2) \alpha_1(s_1) \\
& = P(o_2 | s_2, \lambda) \sum_{s_1} P(s_2 | s_1, \lambda) P(o_1, s_1 | \lambda) \\
& = P(o_2 | s_2, \lambda) \sum_{s_1} P(o_1, s_1, s_2 | \lambda) \\
& = P(o_2 | s_2, \lambda) P(o_1, s_2 | \lambda) \\
& = P(o_1, o_2, s_2 | \lambda)
\end{split}
\\
\begin{split}
\alpha_3(s_3) & = \ldots \\
& = P(o_1, o_2, o_3, s_3 | \lambda)
\end{split}
\end{align}
于是猜想：
\begin{equation}\label{alpha_p_relation}
\alpha_t(s_t) = P(o_1, o_2, \ldots, o_t, s_t | \lambda) \quad \text{当} 1 \le t \le T
\end{equation}
可用数学归纳法证明。假设$\alpha_t(s_{t - 1}) = P(o_1, o_2, \ldots, o_{t - 1}, s_{t - 1} | \lambda)$成立，则有：
\begin{equation}
\begin{split}
\alpha_t(s_t) & = B(s_t, o_t) \sum_{s_{t - 1}} A(s_{t - 1}, s_{t - 1}) \alpha_{t - 1}(s_{t - 1}) \\
& = P(o_t | s_t, \lambda) \sum_{s_{t - 1}} P(s_t | s_{t - 1}, \lambda) P(o_1, o_2, \ldots, o_{t - 1}, s_{t - 1} | \lambda) \\
& = P(o_t | s_t, \lambda) \sum_{s_{t - 1}} P(o_1, o_2, \ldots, o_{t - 1}, s_{t - 1}, s_t | \lambda) \\
& = P(o_t | s_t, \lambda) P(o_1, o_2, \ldots, o_{t - 1}, s_t | \lambda) \\
& = P(o_1, o_2, \ldots, o_t, s_t | \lambda)
\end{split}
\end{equation}
再结合初始条件\eqref{induction_init}，可证明\eqref{alpha_p_relation}。类似可证：
\begin{equation}\label{alpha'_p_relation}
\alpha'_t(s_t) = P(o_1, o_2, \ldots, o_{t - 1}, s_t | \lambda) \quad \text{当} 1 \le t \le T
\end{equation}

当然也可以直接利用结论\eqref{alpha_p_relation}证明：
\begin{equation}
\begin{split}
\alpha'_t(s_t)
& = \frac{\alpha_t(s_t)}{B(s_t, o_t)}\\
& = \frac{P(o_1, o_2, \ldots, o_t, s_t | \lambda)}{P(o_t | s_t, \lambda)}\\
& = \frac{P(o_1, o_2, \ldots, o_{t - 1} | s_t, \lambda) P(o_t | s_t, \lambda) P(s_t | \lambda)}{P(o_t | s_t, \lambda)}\\
& = P(o_1, o_2, \ldots, o_{t - 1}, s_t | \lambda)
\end{split}
\end{equation}

式子\eqref{alpha_p_relation}和\eqref{alpha'_p_relation}就是$ \alpha_t $和$ \alpha'_t $的概率意义。参考文献？？？中的推导则是先定义了$ \alpha_t $的概率意义，然后再推出这组函数之间的递推关系的。本文采取了不同的推导路径，为的是让$ \alpha_t $的引入显得不那么magic。

\subsection{后向算法}
上一小节是从$ s_1 $开始重排，称之为前向算法(forward algorithm)，但也可以从$ s_T $开始，从后到前，称之为后向算法(backward algorithm)：
\begin{equation}\begin{split}
P(o | \lambda) ＝ \sum_{s_1} \pi(s_1) B(s_1, o_1) \prod_{t = 2}^T \sum_{s_t} A(s_{t - 1}, s_t) B(s_t, o_t)
\end{split}\end{equation}

与前向算法相同，可以“截断”到$ B(s_t, o_t) $左边或者右边，分别得到\textbf{一组}函数$ \beta_t $：

\begin{align}
\label{beta_t_definition}
\beta_t(s_t) & = 
\begin{cases}
	B(s_T, o_T) & \text{当} t = T\\
	B(s_t, o_t) \mathlarger{\prod}_{u = t + 1}^T \mathlarger{\sum}_{s_u} A(s_{u - 1}, s_u) B(s_u, o_u) & \text{当} 1 \le t \le T - 1
\end{cases}
\\
\label{beta_t_recursive}
\beta_t(s_t) & = B(s_t, o_t) \sum_{s_{t + 1}} A(s_t, s_{t + 1}) \beta_t(s_{t + 1}) \quad \text{当} 1 \le t \le T - 1
\\
\label{beta_t_final_shot}
P(o | \lambda) & = \sum_{s_1} \pi(s_1) \beta_t(s_1)
\\
\label{beta_p_relation}
\beta_t(s_t) & = P(o_t, o_{t + 1}, \ldots, o_T | s_t, \lambda) \quad \text{当} 1 \le t \le T
\end{align}
和\textbf{一组}函数$ \beta'_t $：
\begin{align}
\label{beta'_t_definition}
\beta'_t(s_t) & = 
\begin{cases}
	1 & \text{当} t = T\\
	\mathlarger{\prod}_{u = t + 1}^T \mathlarger{\sum}_{s_u} A(s_{u - 1}, s_u) B(s_u, o_u) & \text{当} 1 \le t \le T - 1
\end{cases}
\\
\label{beta'_t_recursive}
\beta'_t(s_t) & = \sum_{s_{t + 1}} A(s_t, s_{t + 1}) B(s_{t + 1}, o_{t + 1}) \beta_t(s_{t + 1}) \quad \text{当} 1 \le t \le T - 1
\\
\label{beta'_t_final_shot}
P(o | \lambda) & = \sum_{s_1} \pi(s_1) B(s_1, o_1) \beta'_t(s_1)
\\
\label{beta'_p_relation}
\beta'_t(s_t) & = P(o_{t + 1}, o_{t + 2}, \ldots, o_T | s_t, \lambda) \quad \text{当} 1 \le t \le T
\end{align}

这些式子的推导过程与上一小节的推导过程很相似，推导出来的形式也是很相似的，故略去不写。

\subsection{小结}
给定一组参数模型$ \lambda = (A, B, \pi) $和一个观察序列$ o = o_1 o_2 \ldots o_T $，要计算$ P(o | \lambda) $，第一个直接的思路是把对$ P(o, s | \lambda) $进行“margin out”，但是发现直接按公式计算复杂度太高。第二个尝试是利用乘法分配律减少重复计算，由此导出了前向和后向算法，而这两者中又都有“截断”到$ B(s_t, o_t) $左边和右边两种做法。希望这样的表述能够让读者更容易把握推导的整体脉络。

单就解决第一个问题而言，前向和后向算法用一个就可以，但是在接下来解决第三个问题即模型训练问题的过程中，需要同时用到$ \alpha_t $和$ \beta_t $，综合起来正是所谓的前向－后向算法(forward-backward algorithm)。

同样地，单就第一个问题，无论是前向还是后向，“截断”到$ B(s_t, o_t) $左边或者右边都可以，但是我们还是把两种可能性都列了出来，目的是为了第三个问题中更为清晰的表述。

\section{第三个问题}

要求最优化参数$ \lambda_0 = \argmax_{\lambda} P(o | \lambda) $，这个问题本身其实是属于用最大似然法优化参数，至于用贝叶斯方法怎么优化参数，并不在本文的讨论范围内。最大似然法一般不直接优化似然函数，而是优化对数似然函数$ \log P(o | \lambda) $，然后对参数$ \lambda $	求导，令等于零，求解……但这在HMM上做不了，因为式子\eqref{P(o|lambda)}很难应用这种方法。事实上，目前并没有HMM参数优化问题的解析解。

HMM的参数训练一般是用Baum-Welch算法，只能保证得到局部最优的参数。它其实就是通用的EM算法框架在HMM参数训练问题上的具体使用。接下来先介绍EM算法，再具体介绍Baum-Welch算法。

\subsection{EM算法框架}

Expectation Maximization(EM)算法是一个通用的算法框架，只有应用在像HMM参数训练这样的具体问题上时



\end{document}